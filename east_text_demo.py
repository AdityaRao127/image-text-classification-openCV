# -*- coding: utf-8 -*-
"""EAST_TEXT_DEMO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BPNnMyxi1Wfe7qMNFmndT8M9gFK7Psok

This notebook has code to convert a pre-trained EAST model to TFLite.


The EAST pipeline is capable of predicting words and lines of text at arbitrary orientations on 720p images, and furthermore, can run at 13 FPS, according to the authors.

EAST was proposed in An Efficient and Accurate Scene Text Detector.

## Setup
"""

import tensorflow as tf
tf.__version__

from imutils.object_detection import non_max_suppression
from google.colab.patches import cv2_imshow
from imutils import paths
import numpy as np
import time
import cv2
import os

"""## Cloning into an existing GitHub repository with the OpenCV text detection EAST algorithm"""

!git clone https://github.com/diewland/text-detection-opencv-east.git

!python /content/text-detection-opencv-east/opencv_ocr_image.py --image /content/text-detection-opencv-east/images/sign.jpg \
	--east /content/text-detection-opencv-east/frozen_east_text_detection.pb

"""## TFLite Model Conversion

If we export the float16 model with a fixed known input shape we can can likely accelerate its inference with TFLite GPU delegate. We can specify the `input_shapes` argument in the `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph()` function to do this. We are going to follow this same principle for other quantization (i.e. int8 and dynamic-range) methods as well.

In order to use integer quantization we need a representative dataset. We will be using the [COCO-Text dataset](https://vision.cornell.edu/se3/coco-text-2/) for this purpose. As the dataset is pretty heavy in size, I have prepapred a small subset of it by sampling 100 training images randomly.
"""

!wget -q https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/releases/download/v0.11.0/coco_text_100.tar.gz
!tar xf coco_text_100.tar.gz

IMAGE_LIST = list(paths.list_images('/content/coco_text_100'))
IMG_SIZE = 320

def representative_dataset_gen():
    for image_path in IMAGE_LIST:
        image = cv2.imread(image_path)
        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
        image = image.astype("float32")
        mean = np.array([123.68, 116.779, 103.939][::-1], dtype="float32")
        image -= mean
        image = np.expand_dims(image, axis=0)
        yield [image]

quantization = "int8" #@param ["dr", "int8", "float16"]
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file='/content/text-detection-opencv-east/frozen_east_text_detection.pb',
    input_arrays=['input_images'],
    output_arrays=['feature_fusion/Conv_7/Sigmoid', 'feature_fusion/concat_3'],
    input_shapes={'input_images': [1, 320, 320, 3]}
)

converter.optimizations = [tf.lite.Optimize.DEFAULT]

if quantization=="float16":
    converter.target_spec.supported_types = [tf.float16]
elif quantization=="int8":
    converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

tflite_model = converter.convert()
open('east_model_{}.tflite'.format(quantization), 'wb').write(tflite_model)

!ls -lh *.tflite
!ls -lh /content/text-detection-opencv-east/frozen_east_text_detection.pb

"""We see a substantial reduction in the sizes of the TensorFLow Lite models compared to the original model size.

## Inference

### Prepare an input image
"""

def preprocess_image(image_path):
    # load the input image and grab the image dimensions
    image = cv2.imread(image_path)
    orig = image.copy()
    (H, W) = image.shape[:2]

    # set the new width and height and then determine the ratio in change
    # for both the width and height
    (newW, newH) = (320, 320)
    rW = W / float(newW)
    rH = H / float(newH)

    # resize the image and grab the new image dimensions
    image = cv2.resize(image, (newW, newH))
    (H, W) = image.shape[:2]

    # convert the image to a floating point data type and perform mean
    # subtraction
    image = image.astype("float32")
    mean = np.array([123.68, 116.779, 103.939][::-1], dtype="float32")
    image -= mean
    image = np.expand_dims(image, 0)

    return image, orig, rW, rH

#image, orig, rW, rH = preprocess_image("/content/text-detection-opencv-east/images/lebron_james.jpg")
image2, orig2, rW2, rH2 = preprocess_image("/content/text-detection-opencv-east/images/highway_text_detection.jpg")

"""### Perform inference"""

def perform_inference(tflite_path, preprocessed_image):
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    input_details = interpreter.get_input_details()

    if input_details[0]["dtype"]==np.uint8:
        print("Integer quantization!")
        input_scale, input_zero_point = input_details[0]["quantization"]
        preprocessed_image = preprocessed_image / input_scale + input_zero_point
    preprocessed_image = preprocessed_image.astype(input_details[0]["dtype"])
    interpreter.allocate_tensors()
    interpreter.set_tensor(input_details[0]['index'], preprocessed_image)

    start = time.time()
    interpreter.invoke()
    print(f"Inference took: {time.time()-start} seconds")

    scores = interpreter.tensor(
        interpreter.get_output_details()[0]['index'])()
    geometry = interpreter.tensor(
        interpreter.get_output_details()[1]['index'])()

    return scores, geometry

#quantization = "int8" #@param ["dr", "int8", "float16"]
#scores, geometry = perform_inference(tflite_path=f'east_model_{quantization}.tflite',
#                                     preprocessed_image=image)

quantization2 = "int8" #@param ["dr", "int8", "float16"]
scores2, geometry2 = perform_inference(tflite_path=f'east_model_{quantization}.tflite',
                                     preprocessed_image=image2)

# We need to have this shape:  (1, 1, 80, 80) (1, 5, 80, 80)
#scores.shape, geometry.shape
scores2.shape, geometry2.shape

#scores = np.transpose(scores, (0, 3, 1, 2))
#geometry = np.transpose(geometry, (0, 3, 1, 2))
#scores.shape, geometry.shape

scores2 = np.transpose(scores2, (0, 3, 1, 2))
geometry2 = np.transpose(geometry2, (0, 3, 1, 2))
scores2.shape, geometry2.shape

"""## Processing the inference results"""

def post_process(score, geo, ratioW, ratioH, original):
    # grab the number of rows and columns from the scores volume, then
    # initialize our set of bounding box rectangles and corresponding
    # confidence scores
    (numRows, numCols) = score.shape[2:4]
    rects = []
    confidences = []

    # loop over the number of rows
    for y in range(0, numRows):
        # extract the scores (probabilities), followed by the geometrical
        # data used to derive potential bounding box coordinates that
        # surround text
        scoresData = score[0, 0, y]
        xData0 = geo[0, 0, y]
        xData1 = geo[0, 1, y]
        xData2 = geo[0, 2, y]
        xData3 = geo[0, 3, y]
        anglesData = geo[0, 4, y]

        # loop over the number of columns
        for x in range(0, numCols):
            # if our score does not have sufficient probability, ignore it
            if scoresData[x] < 0.5:
                continue

            # compute the offset factor as our resulting feature maps will
            # be 4x smaller than the input image
            (offsetX, offsetY) = (x * 4.0, y * 4.0)

            # extract the rotation angle for the prediction and then
            # compute the sin and cosine
            angle = anglesData[x]
            cos = np.cos(angle)
            sin = np.sin(angle)

            # use the geometry volume to derive the width and height of
            # the bounding box
            h = xData0[x] + xData2[x]
            w = xData1[x] + xData3[x]

            # compute both the starting and ending (x, y)-coordinates for
            # the text prediction bounding box
            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
            startX = int(endX - w)
            startY = int(endY - h)

            # add the bounding box coordinates and probability score to
            # our respective lists
            rects.append((startX, startY, endX, endY))
            confidences.append(scoresData[x])

    # apply non-maxima suppression to suppress weak, overlapping bounding
    # boxes
    boxes = non_max_suppression(np.array(rects), probs=confidences)

    # loop over the bounding boxes
    for (startX, startY, endX, endY) in boxes:
        # scale the bounding box coordinates based on the respective
        # ratios
        startX = int(startX * ratioW)
        startY = int(startY * ratioH)
        endX = int(endX * ratioW)
        endY = int(endY * ratioH)

        # draw the bounding box on the image
        cv2.rectangle(original, (startX, startY), (endX, endY), (0, 255, 0), 2)

    # show the output image
    cv2_imshow(original)

#original = cv2.imread("/content/text-detection-opencv-east/images/lebron_james.jpg")
#post_process(scores, geometry, rW, rH, original)

original2 = cv2.imread("/content/text-detection-opencv-east/images/sign.jpg")
post_process(scores2, geometry2, rW2, rH2, original2)

#this occurs cause the pixel locations are meant for the highway sign, not for the stop sign

original2 = cv2.imread("/content/text-detection-opencv-east/images/highway_text_detection.jpg")
post_process(scores2, geometry2, rW2, rH2, original2)